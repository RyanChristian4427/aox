1.11: 2006-09-24

  CONDSTORE goes here.

  Update the installer manpage to match the code.

  0 bytes in every codec.


1.12: 2006-10-23

  Implement rest of Lemonade profile in time for the interop.

  What here? LDAP AB? Webmail?

  Sitewide mail retention policy, even if we don't have anything more
  specific yet.


1.13: 2006-11-20


1.14: 2006-12-18


1.15: 2008-01-29

  <arnt> spend a little time with notes. spread that across the next
  five releases.

  - SIEVE
  
    - What modules do we need?
    - Sensible integration with some anti-spam filter
  
  - Full-text search
  
  - aox backup/restore (or similarly helpful procedure)
  
  - LDAP authentication support
  
  - Memory use improvements
  
  - Message retention policy
  
    - Soft-quota/archival stuff too?
    - Message arrival tag (for archiving)
  
  - IMAP extensions
  
    - SORT, THREAD, CATENATE, URLAUTH, CONDSTORE, ANNOTATE, METADATA
  
  - SMTP submit + BURL
  
  - httpd
  
    - TLS support
    - Administration
      - users/mailboxes/permissions/views
    - Read-only archive pages
  
  - aoximport could be much faster (disable indices, commit several
    thousand messages at a time in one big transaction), but it's a
    fair bit of work (unless aoximport has a private Injector)
  
  - Autoresponder
  
  - Miscellaneous:
  
    - aox reinject
    - Indexing for PDF/DOC bodyparts
    - Database replication support (local mirror)
    - Clean up orphaned bodyparts without locking the table
    - Split the injector transaction into fast and slow parts(?)


Undefined codepoints

  When a codec encounters an undefined codepoint, it should substitute
  U+FFFE (or FFFD/FFFF?) and continue parsing after setting a flag that
  later allows parseBodypart() to upgrade (or is that "downgrade"?) the
  entire message to UTF-8 for subsequent retrieval.

  This is done as a special case for GB2312/ISO-2022-JP right now.


Better caching.

  FieldNameCache etc. can use less memory by reading all field and
  flag names into RAM at startup. That way, the memory used won't be
  scattered across many blocks.


Port 587

  Submit is not very different from SMTP and LMTP, for which we have
  servers. The backend is different. We don't have the backend. But as
  soon as we have a sieve, we have the backend, because it's the same
  code that redirect needs.

  Once we have 587, I want to implement RFC 2852, RFC 388[5-8] and
  draft-vaudreuil-futuredelivery. I so much want 2852 and
  futuredelivery for myself.


Message tracking

  RFCs 3885-8 specify ways to track messages that have been sent. We
  can implement that fairly easily.

  If we route outgoing mail via a smarthost, that smarthost has to
  support MTRK in order for tracking to work well.

  We can track mail provided that at least one of these is true:

  - we deliver directly to the end server (looking at received fields
    for mail from the same address can help ascertain that the best mx
    really is that)

  - we deliver via an MTRK-capable server

  - we deliver into our own database

  Sounds likely to be true maybe 80-90% of the time.

  If none are true, we can at least say, easily, where we delivered,
  when, and why.

  We could implement the tracking protocol (and I'd write a query blah
  in mailchen), and also provide a query interface via the web.


aox reinject

  Said command would search for unparsable messages in the DB, try
  each to see whether it can be parsed by the current version, and if
  so, inject the message in the relevant mailbox and delete the
  "cannot parse" message. While it does so, reinject should emit lots
  of progress information. Lots.

  Some problems there - what if the correct message would be sieved
  differently? do we have all the information we need, e.g.
  internaldate? how can we search for these bodyparts in the first
  place?


Bounces and DSNs

  Mail is currently fairly reliable. There is one big exception:
  Bounces aren't 100% parsable. But generally, if you work hard, you
  can know whether a message was delivered or not, and mostly they are
  delivered.

  So we benefit from converting the most common nonstandard bounces to
  DSNs, and then treat them as DSNs.

  For nonstandard bounces (like those of qmail) we identify the
  message by trying hard, do some hacky parsing, use the bounce
  (excluding trailing message) as first part of the DSN multipart,
  cook up a new DSN report based on the parsing, and save
  text/822-headers as a third part.

  Then, searches that tie bounces together with messages sent work
  even better.

  (Another trick we can/should use is to see whether the host we
  deliver to seems to be the final destination based on earlier
  (answered) messages.)


Memory use for common operations

  Some use _vastly_ too much memory. I saw a single IMAP FETCH for a
  mere 4200 messages use 173MB yesterday.

  The most elegant way to solve that would be to supervise memory
  usage for a known sequence. Inject these ten thousand messages,
  check memory use (via a hack), connect to the imap port run this and
  that, check again, connect to the imap port run this and that, check
  again, connect to the imap port run this and that, check
  again... just a bunch of checks. Ignore output.


Splitting large fetches

  Fetch might split large fetches into sets of at most x messages,
  e.g. 256 or 512, to limit the peak amount of memory used to cache
  messages.

  Fetch 512. Send responses. Drop the memory. Repeat.

  When we start a fetcher, Mailbox would have to return all the
  Messages. Each Fetcher would have to keep the Messages it's going to
  write, the Mailbox would keep no pointers, and the Fetch would have
  pointers to the objects it will need.

  So: Fetch asks Mailbox for the headers of messages 1-300. Mailbox
  looks around and asks its existing Fetchers whether any of those
  have pointers to those Messages. It completes the set by creating
  new Message objects, and tells the HeaderFetcher to fetch the
  messages for that set.

  When the HeaderFetcher has done so, it notifies the EventHandler,
  ie. the Fetch, and the Fetch sends responses and may or may not ask
  for more messages.

  Both Fetch and Fetcher must be careful to drop Message pointers once
  it has used the objects.

  The GC logic will pick up on the memory use and collect the Message
  objects speedily. No more 173MB fetches.

  (Opera remains a problem. It sends 'FETCH 1:* FLAG' every minute or
  so.)


Deleting mailbox has unclear semantics now

  At present, we don't allow a mailbox to be deleted until all
  undeletable messages from that mailbox have been sent to the great
  mailbox in the sky.

  We might want to change that, but what then? What if someone wants
  to rename a new mailbox to the name that used to be taken by this
  mailbox? Or creates ditto? What should happen to views onto this
  mailbox?


COPY is slow

  Inject 11k messages, copy with thunderbird, watch why it fails.


imapd/handlers/acl.cpp

  Different tasks, some shared code, same file. Separate this out into
  different classes inheriting something. Then add the right sort of
  logging statement to the end of parse().


Opera often can't expunge

  Trace it once or twice, guess what's wrong.


ocd needs work (mike@waspfactory.org)

  1. The code (along with user/mailbox) needs cleaning up.
  2. Should synchronise field names etc.

  Note that we can drop ocd in the non-cluster case starting with
  1.07. Simply not start it.


Don't duplicate the contents of address_fields in header_fields.

  The Injector should avoid inserting address fields into header_fields,
  and the HeaderFetcher should assemble addresses from address_fields.


AddressCache and FieldNameCache are utter crocks, and must die.


Consider RFC2231:

  2231 says: IMAP4 [RFC-2060] servers SHOULD decode parameter value
  continuations when generating the BODY and BODYSTRUCTURE fetch
  attributes.

  Hopefully we don't need to care about this at all.

  Arnt's opinion: We're better off doing that at parse time. There's
  also some outlook breakage to support - it uses 2047 when it
  shouldn't.


ANNOTATE support

  Annotations are good. We want them. They're easy for us. By now the
  draft should be stable, so we can implement whenever we want to and
  have the time.

  Status:
  - schema changes: done
  - session state: done
  - SELECT/EXAMINE: done
  - FETCH: written, not tested, probably broken
  - STORE: done, currently broken
  - COPY: done (untested for lack of other support)
  - APPEND: can call code currently in STORE
  - SEARCH: done
  - SORT


METADATA

  Ditto. Separate point so that we can do one without the other.


CONDSTORE

  RFC 4551. Needed. What we need to do:

  - client capabilities, so we can send untagged responses (done)
  - OK untagged responses to SELECT/EXAMINE (3.1)
  - STORE UNCHANGEDSINCE/MODIFIED
  - FETCH CHANGEDSINCE (done)
  - FETCH MODSEQ (done)
  - SEARCH MODSEQ (partly done)
  - Selector changes (done)
  - SEARCH response
  - STATUS HIGHESTMODSEQ (done)
  - SELECT/EXAMIME CONDSTORE (done)
  - updating the modseq in store
  - updating the modseq for non-peek fetches, if unseen
  - schema update (done)
  - testing all of the above (NOT done)

  The search is tricky, since the select now needs to return something
  more than just UIDs. How do we make that work together with views?


Autoresponder

  We'll need one, nicely programmable, connected to our sieve.

  I was reminded of this by Google's wording:

  Thank you for writing to Google. This automated response is just to
  let you know that we've received your email, and you'll hear from us
  soon.

  Thank you for using Google.

  Regards,
  The Google Team


Search blocks too hard

  From the log:

    16:08:50.900 IMAP Command: 'gg uid search'
    16:08:50.900 First line: gg uid search uid 33788:*
    16:08:50.907 Search for uid>=33786 and uid<33789
    16:08:50.907 Executing IMAP command 'gg uid search'
    16:08:55.121 Search considered 4638 of 4638 messages using cache
    16:08:55.123 Executed IMAP command 'gg uid search' in 4216ms
    16:08:55.123 Result: OK done

  The server was busy for a long time with that search. It's time to
  optimise Selector::consider a little bit so it doesn't spend so much
  time looking at messages that'll fail. Three steps: 1) propagate
  possible uid ranges towards root. 2) if the range at root is too
  big, skip the cache pass. 3) only consider messages in the range.


Message arrival tag

  Once annotate is done, we want a tag, ie. a magic annotation which
  stays glued to the message wherever it goes, even after copy/move.

  We also want a way to store the original RFC822 format somewhere
  inside and/or outside the database, indexed by the arrival tag
  identifier. It's good if the tag is split, so we can have "x-y"
  where X is the CD/DVD number and Y is the file on the CD/DVD. Or
  something like that.


Message Retention Policy Framework

  A lot of sites will want explicit policies regarding what mail may
  not be deleted, what may be deleted, and what must be deleted. We
  can support that well.


C/R

  C/R sucks. But it has its uses, so we can benefit from implementing
  it somehow. Here are some classes of messages we may want to treat
  specially:

  - replies to own mail
  - messages in languages not understood by the user
  - mail from previously unknown addresses
  - mail from freemail providers
  - vacation responses from unknowns
  - messages likely, but not certain to be out-of-office-autoreply

  The questions are: How can we ensure that we almost never challenge
  real mail, while simultaneously challenging most/all messages that
  don't come from valid senders? How can we provide suitable
  configuration?


We should figure out some way to not store plaintext passwords.

  In practice, the only way is to subcontract the dirty work to an
  LDAP server. We act as a MITM while doing SASL authentication and
  that's all.


Using rrdtool

  What could we want to graph with rrdtool? Lots.

  - RAM used
  - CPU seconds used
  - database size
  - messages in the db
  - average response time
  - 95th percentile response time
  - messages injected
  - messages per user
  - message size per user
  - logged-in users
  - concurrent connections (per protocol)
  - logins (per protocol)
  - login failures (per protocol)

  More?

  The natural way to do it would be to have archiveopteryx collect
  some/most variables in RAM, and provide a query via aox+ocd. some
  things (messages/message size per user at least) are better done as
  a db query from aox directly.

  'aox show statistics'?


Optimising common cases commonly helps

  Now that the lmtp and pop/imap/webmail servers share address space,
  an injected message can also be short-circuited into the Mailbox if
  there are any Sessions on the Mailbox. Or perhaps if there are any
  watched Sessions on the Mailbox.

  When that's done, a message can be handed out to IMAP/POP/Webmail
  clients without database activity. More speed, less DB work.

  In order for this to work well, we must have better caching, so the
  Mailbox doesn't fill up too much. Necessary for other reasons
  too. See separate item.


Caching Messages in Mailbox

  Instead of storing Messages in Mailbox, we could cache them. The
  Mailbox could throw them away regularly. When we want to fetch one,
  we could call a Mailbox function to return a pointer to the object,
  and the caller would be responsible for keeping the pointer around
  until any DB activity is complete and the Message object can be
  processed.

  Messages could be discarded by Mailbox if they have all the
  information for which the Mailbox is currently fetching
  something. Ie. if the only Fetcher currently existing is a
  FlagFetcher, then a Message can be discarded if it hasFlags().

  The number of Messages in RAM may follow a Poisson
  distribution. Sounds likely. If so, the cache-clearing algorithm
  must work well both for the very small and very large parts of the
  graph.


SASL NTLM authentication

  It may be odd and undocumented, and it may not be as strong as
  DIGEST-MD5, but it's implemented in Certain Clients ;)

  http://www.innovation.ch/java/ntlm.html seems to be a reasonable
  description. Cyrus also implements it.


SASL LOGIN authentication

  Implemented in Certain Clients ;)

  There's a draft by Ken Murchison.


httpd segfaults when you click on a message (Harri).


Make the httpd do archive pages

  Shouldn't we be most of the way there? Need partial result pages and
  real searching, though.

  Notify denis@startsiden.no when external testers might want to look
  at it.


archives.oryx.com

  Mostly need httpd archiving.


Miscellaneous cleanups:

  * Get rid of Blah::setup wherever possible
  * Avoid unnecessary header file inclusion
  * Enable the #ifdef'ed out tests
  * Do the "uint characters[256]" thing to simplify parsing.


DELETE of mailboxes in inappropriately qualified (security problem)

  At present, the user always needs to have all three rights, and
  there's a race condition against append/copy. It would be better to
  grab the uidnext lock and do more fine-grained checking:

  1. Require DeleteMessages unless all messages have \deleted.

  2. Require Expunge if there are any messages at all.

  The race condition means it's possible to append to a mailbox
  while it's being deleted, and the message will reappear if the
  mailbox is later recreated.


Split the folder view into pages.

  The injector needs to do threading to make the paging possible.


A failing parse message should invalidate the prepared statement cache.


Look into supporting the MySQL protocol after 1.0


Orphaned bodyparts should be cleaned up somehow.

  We need a bool on bodyparts, saying whether it's okay to delete this
  bodypart or not: "perhaps" and "no".

  The injector sets it to "no way" when it does 'insert into
  bodyparts'. vacuum -b works in 2-3 steps. First it deletes the
  already-marked and unused bodyparts:

    delete from bodyparts where id in (
      select id from bodyparts b left join part_numbers p on (b.id=p.bodypart)
        where bodypart is null and deletable='t');

  If the delete fails, vacuum -b responds by cleaning up so it'll work
  next time:

    update bodyparts set deletable='f' where id in (
      select id from bodyparts b join part_numbers p on (b.id=p.bodypart)
        where deletable='t');

  Finally it readies the database for the next crontab run:

    update bodyparts set deletable='t' where id in (
      select id from bodyparts b left join part_numbers p on (b.id=p.bodypart)
        where bodypart is null and deletable='f');

  No mailbox lock required.


We need mUTF-7 support.


Write better field wrapping code

  HeaderField::wrap is terribly hackish right now. Besides, ::unwrap()
  should become the higher-level parser's job.


We should be able to use a read-only local database mirror.

  That way, we can play nicely with most replication systems.

  The way to do it: add a new db-mirror setting pointing to a
  read-only database mirror. all queries that update are sent to
  db-address, all selects are sent to db-mirror. db-mirror defaults to
  db-address.


canonical and deliver need severe cleaning up.


Write a valgrind skin that should help us to diagnose memory abuse.


Read the GiST papers and look at tsearch2 again.


migrator should work without indices

  If the database is otherwise idle, the migrator could disable indices
  and perhaps transactions, and go much faster.


Add Maildir support to the migrator.

  Currently works, with two exceptions: Submaildirs don't work, and
  courier's extended flags don't work.


We should test multipart/signed and multipart/encrypted support.

  We must add a selection of RFC 1847 messages to canonical, and make
  sure they survive the round trip. No doubt there will be bugs.


Something confuses Address::uname() and Address::name()

  As a result, we send unlabelled 8-bit content in display-names if
  the name cannot be expressed in either 8859-1, 8859-2 or koi8-r.

  I think the underlying reason is that we're not using UString, so
  the compiler's type checking does nothing for us.

  This is easy to demonstrate with 22 right now, but it won't be
  shortly - I'll make a separate change which accidentally obliterates
  the difference.


We should store bodyparts.text for PDF/DOC.

  We need non-GPLed code to convert PDF and DOC to plaintext. We already
  use the code from Mailchen to convert HTML.


CAcert instead of self-signed certificate?

  www.cacert.org offers free certificates and seems to be an
  up-and-coming thing.  In a few months, we may want to add 'obtain
  valid cert from cacert'.

  Btw, cacert.org is a very nice place to get praise and a link from. High
  nerd factor. If we have the first really-easy-to-use interface to
  get cacert interfaces into a program, they'll do that, no doubt.


Switch to using named constraints everywhere.


Default c-t-e of PGP signatures

  Right now we give them binary. q-p or 7bit would be better, I think.

  What other application/* types are really text?

  From a conversation the other day: we could avoid base64 encoding an
  entity whose content-type is not text if it contains only printable
  ASCII. I don't know if it's worth doing, though.

  The problem with doing that is that it treats sequences of CR LF, CR
  and LF as equivalent. An application/foobar object that happens to
  contain only CR, LF and printable ASCII can be broken.


The IMAP SORT extension seems really easy

  Squirrelmail wants it. Strongly. And it's not terribly difficult to
  implement, to put it mildly.

  Interacts with ANNOTATE.


String allocation is about half as fast as it could be

  String and StringData are allocated independently. String could
  override the allocation method and allocate sizeof( String ) +
  sizeof( StringData ), and then forcibly set d.

  The same applies to other classes which we instantiate very often.


Something like soft-bounce might just be useful

  enabled: smtp/lmtp always give 4xx errors. disabled: either 4xx or
  5xx depending on the type of error.


recognising spam

  The good spam filters now all seem to require local training with
  both spam and nonspam corpora. We can do clever stuff... sometimes.

  Instead of filtering at delivery, we can filter when a message
  becomes \recent. When we increase first_recent, we hand each new
  message to the categoriser, and set $Spam or $Nonspam based on its
  answer.

  This lets the categoriser use all the information that's available
  right up to the moment the user looks at his mail.

  We can also build corpora for training easily. All messages to which
  users have replied are nonspam, replies to messages from local users
  are nonspam, messages in certain folders are spam, messages with a
  certain flag are spam.

  We can connect to a local server to ask whether a message is spam.
  They seem to work that way, but with n different protocols.


predictive db fetching

  This change adds a hack to imapd/handlers/fetch.cpp, to defeat
  pessimal clients like thunderbird.

  It's inchoate.

  It seems to me that the proper way would be to record something in
  MessageFetcher and extend fetches there. For example, keep a cache
  of (fetch type, mailbox, uid, uid range) tuples. Each fetch creates
  one such tuple. If the fetch includes the uid, it's expanded by the
  uid range. If the fetch is not predicted by any tuple, add a new
  tuple for the n UIDs following the highest requested UID.


Replace Mailbox::isHome with something correct.

  It should look for a mailbox whose owner is different from its
  parent's owner.


TLS client support (smtp, postgresql)


We MUST support HTTPS for the webmail interface.


"Writing Secure Code"

  We have a page about security, /mailstore/security.html, and a
  section of the mailstore.7 man page mentions it too.

  We need to look at ISBN 0735617228 and improve security.html with
  points from it. It could also be that we'll improve the code itself.


udoc stuff:

  1. Support a single level of nested classes. (What file names to use
     for output?)
  2. Support enum annotation.
  3. Suppress empty <p>, duplicate anchor names in output.


Udoc web pages chores

  Add "Related Pages" etc. Clarify where background.html
  fits. usage.html is an orphan now; should it become a manpage?


Play with PITR and write /ams/pitr.html


Update doc/recorder.man

  Must decide whether to use .SS or .IP, etc.


Document IPC structure

  Some man page, or some web page, or both, should say who's
  connecting to who and why.


Add a web page about the charset encoding.

  It's a novel and good algorithm, so we can make a good page about
  it. We also can link to data sources there.

  The documentation for Codec::byString() should mention that page's
  URL.


Make a web page about our licensing

  Not sure what to say there. the purpose of the page would be to
  direct people to one of the two others, really. and to be linked to
  from the home page.


The "Database" link on home page

  Where should it go? People might click it wondering why to use a
  database instead of flat files and wanting to know what we do with
  databases.


Interop pages

  A variety needed.


Protocols supported

  A list detailing what our general idea is (internet standard,
  interoperability, utility) and linking to each actual protocol page.

  The URL for this is difficult.


Search ourselves, not via google

  Or maybe farm that out, get google to search with an approximation
  of our design. http://www.google.com/faq_freewebsearch.html may be
  interesting.


Fix shutdown

  There's no practical way to shutdown using ocd/ocadmin. Perhaps this
  is another aspect of the problem above.


Resource limits on memory allocation


Logging to syslog

  logfile=syslog:local2 could mean "log to syslog with the local2
  facility". that's ugly syntax. parsing and blah.

  if we implement this, the logd HUP handler can switch to syslog if
  it can't reopen the logfile.


aox restart and aox start don't check all they could

  Improvements:

  1. aox restart and aox start should bind to the IP addresses specified
     by all the relevant servers (but not the ports) and try to
     connect to the ports.

  2. aox restart can issue 'aox start' when the last established
     connection dies. currently it calls sleep(1).

  3. aox start can expect all the connects to fail. if a connect
     succeeds, someone else is using that port, so our server won't.

  4. Both of them should expect all the binds to succeed. if not,
     mailstore.conf contains a bad address.


Make User::create/remove work like Mailbox::create/remove. (AMS)


Make sure UIDs never exceed 2**31 (and break getInt).


Rendering webmail HTML is presumably good, but...

  Knowledge is better than presumption.

  Webmail services have a lot of known holes. Cross site scripting and
  suchlike. We can add known sploits/demos to our rendering tests, and
  check that we're not vulnerable.

  Two I saw today: http://www.sec-consult.com/232.html and
  http://www.sec-consult.com/233.html


Making injection faster.

  If we use a separate transaction to acquire the UID, then we hold
  the mailbox lock for (how much?) less time.

  We hold the lock MUCH shorter. Just enough to 'select x for update',
  'update x' and commit, which is pretty close to the shortest possible
  database transaction.

  Pro: Injection can use 100% of the database server. It's no longer a
  bottleneck. If the database server has 16 CPUs, then we can use 16 CPUs
  for whatever load we have. No locks can cause CPUs to sit around
  waiting (almost).

  Contra: For a short while, uidnext has moved but the message has not
  appeared in the database. In that time, the imapd can currently infer
  that the message has existed and been expunged, which sits badly with
  imap caching semantics. But I think we can deal with that, e.g. by
  saying that ocd is updated last, and a lower value seen via ocd wins
  over a higher value in the database. Something like that.

  A note to the note: If we use full-text indexing and inject
  bodyparts before the start of the transaction (ie. not using
  savepoints), then we'll be able to load the db server nicely just by
  injecting bodyparts.


When TlsServer dies, imaps (993) must go away

  It doesn't at the moment. Anything else?


Faster mapping from unicode to 8-bit encodings

  At the moment, we use a while loop to find the right codepoint in an
  array[256]. Mapping U+00EF to latin-1 requires looping from 0 to
  0xEF, checking those 239 entries.

  We could use a DAG of partial mappings to make it faster. Much
  faster. Mapping U+20AC to 8895-15 would require just one lookup: In
  the first partial table for 8859-15. Mapping U+0065 to 8859-15 would
  require three: In the first (U+20AC, one entry long), in the
  fallback (U+00A0, 96 entries long) and in the last (U+0000, 160
  entries long).

  Effectively, 8859-15 would be a first table of exceptions and then
  fall back to 8859-1.

  The tables could be built automatically, compiled in, and would be
  tested by our existing apparatus.


Multipart/signed automatic processing

  We could check signatures automatically on delivery, and reject bad
  signed messages.

  The big benefit is that some forgeries are rejected, even though the
  reader and the reading MUA doesn't do anything different.

  The disadvantage is that we (probably?) can't verify all signatures,
  which gives a false sense of security for the undetactable forgeries.

  In case of PKCS7, it's possible to self-sign. Those we cannot
  check. In that case we remove the signature entirely from the MIME
  structure, so it doesn't look checked to the end-user.

  PGP cannot be checked, except it sort of can. We can have a small
  default keyring including the heise.de CA key and so on, and treat
  that as root CAs, using the keyservers to dig up intermediate keys.


PGP automatic processing

  Apparently there are five different PGP wrapping formats. We could
  detect four and transform them to the proper MIME format.


imap/hackedsize#2 doesn't work.

  - The Received field added by smtpd changes.
  - The x-obliterate breaks ocd, so the select response is weird.


LDAP server/address book

  Joins rock. We can join ALL OUR TABLES and get INCREDIBLE
  FLEXIBILITY AND POWER.

  LDAP AB can provide several address books based on the archive:

   - all addresses visible anywhere
   - all addresses in your mailboxes
   - all addresses in mailboxes to which your groups have access
     (the join for this is a little hard)
   - all v-card bodyparts
   - all v-cards in your mailboxes
   - all v-cards in mailboxes to which your groups have access
     (I suppose one might even say "breathtakingly complex join")

  Admins can enable/disable each of those depending on site policy.

  http://www.onlamp.com/lpt/a/3310 may be helpful wrt i14y testing.
  thunderbird should do what it says.


Groups and ACL

  We don't need groups as of now, but they do have their handy-dandy
  aspects. If we add groups, see the LDAP AB item.


Get rid of FORTRAN typing

  Instead of using UString, we sometimes use String and "assume it to
  be UTF-8", except that sometimes we forget to assume that. Fix that.
  Also unfuck HeaderField::encode{Word,Phrase} in a different way -
  they use different tests for whether 2047 encoding is necessary, and
  I don't think either is right.


VIEW CREATE seems unpredictable

  Occasionally, the \view flag doesn't appear quickly on the created
  mailbox. This may be due to OCClient - I recent saw a mailbox update
  its UIDs very lage (long after append had reported the new UID to
  the IMAP client).


Plugins

  It's not given that we want to accept all mail. If we don't, who
  makes the decision? A sieve script may, and refuse/reject mail it
  does not like. And a little bit of pluginnery may. I think we'd do
  well to support the postfix plugin protocol, so all postfix policy
  servers can work with aox. (All? Or just half? Doesn't postfix have
  two types of policy plugins?)

  We may even support site-wide and group-wide sieve scripts and
  permit a sieve script to invoke the plugin. A sieve statement like
  this?

     UsePolicyServer localhost 10023 ;


INTHREAD

  A new search-key which takes another search-key. If the subsidiary
  search-key returns true for a message, INTHREAD returns true for all
  messages in that thread.

  Problems: Which threading algorithm? More than one? How to advertise
  which one(s) are possible?

  Possibilities: Combined with THREAD, it allows neat things.


URLAUTH (RFC 4467)

  - URLAUTH capability.
  - Per-user and per-mailbox randomly-generated access keys stored
    in a new table. (128 bits of entropy (?))
  - Authorization token: 128+ bits (should identify token-selection
    mechanism).
  - We need SHA-1 code (cryptlib has some).
  - How do we handle expiry dates? Another table?
  - We need an IMAP URL parser.
  - RESETKEY
  - GENURLAUTH
  - URLFETCH


"make install" doesn't create /var/run/oryx

    Perhaps the installer should.


tlsproxy doesn't complain about PEM keys

    There should be clear diagnostics for that case.


Todo list: AMS

  - Figure out why penne/test fails, and setup whip tests with the
    new-style database.
  - Fix the failing tests inspired by Harri's message.
  - Figure out why tlsproxy segfaults on Thomas's box.
  - Go through ANNOTATE and make sure we have and test everything.
  - Update to Postgres 8.1.x on kalyani (+rusage.diff)
  - aox upgrade schema should say something comforting.
  - Update the schema for condstore.
  - Fix imap/view and the recent problems.
  - Figure out the Injector upper bound on Injector speed.
  - Log single-row query values?
  - Use writev in Buffer::write.
  - canonical problems.
  - Poke hdp and jd@commandprompt.
  - Revive dbtest
  - Investigate Scalix/Zimbra/TeamXchange.
  - Make sure LIKE parameters in selector.cpp (whereAddressField in
    particular) are [\%_]-escaped as in q().
  - Why does Flag::find( "\\seen" ) return 0 if used in Session?
  - Make Schema::check independent of upgrade so that we can revoke
    update on mailstore from aox.
  - Restart Postgres while we're running, and see what happens. We
    should be able to cope with that gracefully, but we don't.



